{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fea631d3-654f-4308-81c0-717927130d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your LangChain API Key:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
      "Enter your OpenAI API Key:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "import getpass  # Secure way to prompt the user for sensitive input (e.g. API keys)\n",
    "import os       # Allows interaction with the operating system, including setting environment variables\n",
    "\n",
    "# Enables LangChain's advanced tracing and debugging features (v2) via LangSmith\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "# Prompts the user to enter their LangChain (LangSmith) API key securely (input is hidden)\n",
    "# This key is required to enable tracing in LangSmith for monitoring and debugging your LangChain runs\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangChain API Key: \")\n",
    "\n",
    "# ‚úÖ Set your OpenAI API Key\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# | output: false\n",
    "# | echo: false\n",
    "# (These two lines are Jupyter cell metadata used to suppress output/echo in some notebook renderers like Quarto)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "# Imports the ChatOpenAI class from LangChain's OpenAI integration package.\n",
    "# This allows me to easily call OpenAI's chat models (e.g., GPT-4o, GPT-3.5) using a consistent interface.\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# Initializes a chat model object using OpenAI's \"gpt-4o-mini\" (a lighter, faster GPT-4 model).\n",
    "# I can now use `model.invoke(\"your message\")` to send messages and receive responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95ed257-52ae-43ed-a5a1-3501643da424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "# Imports the HumanMessage class from LangChain.\n",
    "# This class is used to define messages coming from the user (you) in a structured way.\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Dan\")])\n",
    "# Sends a list containing one HumanMessage to the model using `.invoke()`.\n",
    "# This simulates a user saying \"Hi! I'm Dan\" to the chatbot.\n",
    "# The model will process this input and return an AI-generated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64a1a069-ae36-4df0-b8cf-fae3e012848c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HumanMessage' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39minvoke([HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms my name?\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HumanMessage' is not defined"
     ]
    }
   ],
   "source": [
    "# NOTE: LLMs like GPT have no built-in memory between calls.\n",
    "# This is a key concept in LangChain:\n",
    "# If you want memory, you have to implement it yourself ‚Äî either by:\n",
    "# 1. Passing the full message history manually every time (like we're doing here), or\n",
    "# 2. Using LangChain tools like `RunnableWithMessageHistory` or `LangGraph` to handle memory automatically.\n",
    "\n",
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c512dc29-757b-4921-a66c-85665a9d567c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Dan. How can I help you further?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 33, 'total_tokens': 46, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_80cf447eee', 'id': 'chatcmpl-BN0O2f9uqDvqODeaFxq0M7qJgJCqW', 'finish_reason': 'stop', 'logprobs': None}, id='run-652e2858-3758-45dd-a5d1-b6b27977deea-0', usage_metadata={'input_tokens': 33, 'output_tokens': 13, 'total_tokens': 46, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "# Import both HumanMessage and AIMessage.\n",
    "# This allows you to simulate a full back-and-forth conversation between a user and the AI.\n",
    "\n",
    "# We're manually passing the full conversation history to simulate memory.\n",
    "# This is necessary because the model does not remember past interactions unless we give it the full context.\n",
    "# In LangChain, memory can be managed manually like this or automatically using `RunnableWithMessageHistory` or `LangGraph`.\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Dan\"),\n",
    "        AIMessage(content=\"Hello Dan! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b7bcfac-0b1a-49cd-9673-0c4989948913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "# Imports an in-memory checkpointer. This stores message history so it persists between interactions.\n",
    "# It's perfect for testing or quick prototyping. For production, you could use SQLite, Postgres, etc.\n",
    "\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "# Imports core building blocks of a LangGraph:\n",
    "# - `START` is the entry point of the workflow.\n",
    "# - `MessagesState` defines the structure of our state (chat messages).\n",
    "# - `StateGraph` is the object used to define the flow of the chatbot.\n",
    "\n",
    "# Step 1: Create a new LangGraph with the message state schema\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Step 2: Define the function that calls the model\n",
    "# This function takes the current conversation state (all messages),\n",
    "# sends them to the model, and returns a new message as a response.\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])  # Send conversation history to the LLM\n",
    "    return {\"messages\": response}               # Return the updated message list to the state\n",
    "\n",
    "# Step 3: Define the nodes and edges in the graph\n",
    "# Add a node named \"model\" that runs our `call_model` function\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Connect the START of the conversation to the \"model\" node\n",
    "workflow.add_edge(START, \"model\")\n",
    "\n",
    "# Step 4: Add memory to the graph\n",
    "memory = MemorySaver()  # This stores the full chat history in memory between turns\n",
    "\n",
    "# Step 5: Compile the graph into a runnable app, passing in the memory\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0acc0703-853d-4438-b16f-a6e3bab92814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a config dictionary to store metadata for the session\n",
    "# The 'thread_id' tells LangGraph which memory stream to use\n",
    "# This enables multi-turn chat history to be stored and retrieved\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"abc123\"  # You can use any unique string here; this will create a unique memory slot or chat bank (it's kind of like a folder - to store stuff)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3ce2c37-2266-42ae-af9a-eb60d805d109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Dan! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Dan.\"\n",
    "# This is your user input ‚Äî what you're saying to the chatbot\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "# Wrap the query in a HumanMessage object and put it in a list.\n",
    "# LangGraph expects messages in list format for multi-turn history.\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "# This sends the message to the LangGraph app.\n",
    "# - `{\"messages\": ...}` is your input state.\n",
    "# - `config` includes your thread_id (\"abc123\"), so LangGraph knows where to store this memory.\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()\n",
    "# Print the last message in the list ‚Äî this will be the AI's response.\n",
    "# The full message history is stored in `output[\"messages\"]`.\n",
    "# `pretty_print()` gives you a nicely formatted version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c2bc78f-45ed-499c-a927-34e7d33f069a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Dan!\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "# This is your follow-up question ‚Äî you're asking the AI to remember what you said earlier.\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "# Wrap the follow-up question as a HumanMessage (LangChain expects structured messages in a list).\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "# This calls the LangGraph app with:\n",
    "# - The new user message\n",
    "# - The SAME `config` (thread_id = \"abc123\"), so it can retrieve the existing memory from the earlier interaction\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()\n",
    "# Print the AI‚Äôs most recent response (the last message in the list).\n",
    "# If memory is working, it should respond with \"Your name is Dan.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4d497b2-0729-4beb-be1e-479683742ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I still don't know your name since you haven't mentioned it yet. If you'd like to share it, feel free!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "# This is a NEW thread ID, different from \"abc123\"\n",
    "# This effectively resets the memory ‚Äî like opening a blank chat window for a new user or session\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "# Wrap the query (e.g., \"What's my name?\") in a HumanMessage and put it in a list\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "# Invoke the LangGraph app using:\n",
    "# - the new user message\n",
    "# - the NEW thread_id in the config ‚Äî LangGraph will treat this as a brand-new chat with no memory\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()\n",
    "# Print the AI's most recent reply ‚Äî you should get a generic response like:\n",
    "# \"I'm sorry, I don't know your name.\" because there's no context in this new thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c4b2307-f76a-4955-b8fc-292b36162117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Dan.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "# This is the original thread ID you used earlier ‚Äî the one where you said \"Hi! I'm Dan.\"\n",
    "# By using this again, you're \"reloading\" that conversation and all the memory tied to it.\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "# Wrap your latest input (e.g., \"What's my name?\") into a HumanMessage object\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "# Invoke the chatbot using:\n",
    "# - your new input\n",
    "# - the original memory context (`thread_id = abc123`)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()\n",
    "# Print the model's response.\n",
    "# Because it's back on the original memory thread, the model should recall your name is Dan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70b68a8e-0df3-4749-a44d-86b679f8d1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Dan.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "# Reconnects to the original chat thread ‚Äî memory of \"I'm Dan\" is still stored here.\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "# Wraps the user‚Äôs current input into a structured format for LangChain.\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "# Sends the message into LangGraph with memory tracking.\n",
    "# Because this thread has past messages, LangGraph retrieves and includes them automatically.\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()\n",
    "# Displays the model‚Äôs latest reply.\n",
    "# You should see a memory-aware answer, like \"Your name is Dan.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d937ff4-6bfb-49af-826d-4aa3a6edd179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# Imports:\n",
    "# - ChatPromptTemplate: used to structure and format prompts in a modular way.\n",
    "# - MessagesPlaceholder: a placeholder that gets replaced with the full conversation history at runtime.\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are James Bond. Respond with wit, charm, and confidence. Keep your answers cool, concise, and in-character.\",\n",
    "        ),\n",
    "        # This is a system message ‚Äî it sets the tone and behavior of the AI.\n",
    "        # In this case, the AI should roleplay and respond like James Bond.\n",
    "\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        # This placeholder will automatically pull in the chat history\n",
    "        # (everything the human and AI have said so far) from the `state[\"messages\"]` when invoked.\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98d33877-10b8-422e-85ff-2b6c1c49ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "# Re-initialize the LangGraph workflow.\n",
    "# We‚Äôre still using MessagesState to represent the conversation history (as before).\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    # This function runs every time a new message comes in.\n",
    "\n",
    "    # highlight-start\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    # This uses the custom ChatPromptTemplate (James Bond version).\n",
    "    # It injects both the system message and the current messages into a structured prompt.\n",
    "\n",
    "    response = model.invoke(prompt)\n",
    "    # Now the model sees the full prompt ‚Äî including personality + history ‚Äî and generates a response.\n",
    "    # highlight-end\n",
    "\n",
    "    return {\"messages\": response}\n",
    "    # Return the new state with the model‚Äôs response added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d714939f-b161-4a26-8649-2a44daa8b199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Dan! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "# New memory thread ‚Äî this is a clean slate conversation.\n",
    "# The chatbot will NOT remember anything from previous threads (\"abc123\", etc.).\n",
    "\n",
    "query = \"Hi! I'm Dan.\"\n",
    "# The user introduces themselves ‚Äî this is the first message in this conversation.\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "# Wrap the query in a HumanMessage so LangChain understands it as a user's message.\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "# Call the LangGraph app:\n",
    "# - It pulls in your James Bond system prompt from the ChatPromptTemplate\n",
    "# - Injects this first user message into that structured prompt\n",
    "# - Runs it through the model\n",
    "# - Saves it to memory under \"abc345\"\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()\n",
    "# Print the latest AI response ‚Äî this should sound like it came straight out of a Bond movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d9fb4ce-15bf-4355-95e0-9aae2b8a947c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Dan. How can I help you, Dan?\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "# You're now testing whether the chatbot remembers what you said earlier in this specific thread.\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "# Wrap the follow-up question in a HumanMessage.\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "# This will:\n",
    "# - Retrieve the full conversation history stored under the current thread_id\n",
    "# - Inject that into the James Bond prompt template\n",
    "# - Send it to the model and get a contextual response\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()\n",
    "# Prints the model‚Äôs response.\n",
    "# If memory is working, it should say ‚ÄúYour name is Jim.‚Äù (or Dan, if that's what you said earlier)\n",
    "# And it should *still* sound like James Bond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "162089e9-d50c-4cc8-ad68-ccd7e4c21454",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        # `{language}` is a placeholder that gets filled in when you invoke the prompt\n",
    "\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        # This still inserts the chat history into the prompt as before\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7031f9c4-278a-44de-84c7-da2d7640c83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence  # Used to define list-like types (e.g., a sequence of messages)\n",
    "from langchain_core.messages import BaseMessage  # Base class for all message types (e.g., HumanMessage, AIMessage)\n",
    "from langgraph.graph.message import add_messages  # Tells LangGraph to track the `messages` field for memory\n",
    "from typing_extensions import Annotated, TypedDict  # Used to define a typed state schema with metadata\n",
    "\n",
    "#Define the state that your LangGraph app will use and persist\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    #A list of all messages exchanged ‚Äî LangGraph uses this to maintain memory\n",
    "\n",
    "    language: str\n",
    "    #Custom input for the prompt template ‚Äî lets the user define the language dynamically\n",
    "\n",
    "#Create the LangGraph workflow using the custom state\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "#Define what the model does each time it's invoked in the graph\n",
    "def call_model(state: State):\n",
    "    #Create a full prompt by injecting the state (messages + language) into the template\n",
    "    prompt = prompt_template.invoke(state)\n",
    "\n",
    "    #DEBUG: Print each item in the generated prompt for inspection\n",
    "    print(\"\\n----- Prompt Preview -----\")\n",
    "    for msg in prompt:\n",
    "        print(msg)  # üëÄ This avoids calling .type or .content in case msg is a tuple\n",
    "    print(\"--------------------------\\n\")\n",
    "\n",
    "    #Send the prompt to the model and get a response\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    #Return the response wrapped in a list so it can be added to memory\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "#Set up the graph structure: START ‚Üí model\n",
    "workflow.add_edge(START, \"model\")  # Defines the order of operations in the graph\n",
    "workflow.add_node(\"model\", call_model)  # Adds the actual logic for what happens at \"model\" step\n",
    "\n",
    "# Set up in-memory persistence to keep track of conversation history per thread_id\n",
    "memory = MemorySaver()\n",
    "\n",
    "#Compile the full app with the current workflow and memory setup\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "51e622c3-eb04-49f2-b001-d9fd93cc173d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Prompt Preview -----\n",
      "('messages', [SystemMessage(content='You are a helpful assistant. Answer all questions to the best of your ability in Spanish.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Hi! I'm Dan.\", additional_kwargs={}, response_metadata={}, id='47341a4b-968c-44b1-b69d-8d6fbcc8626e')])\n",
      "--------------------------\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "¬°Hola, Dan! ¬øC√≥mo puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "# This thread ID ensures that all messages and memory stay scoped to this conversation session.\n",
    "\n",
    "query = \"Hi! I'm Dan.\"\n",
    "language = \"Spanish\"\n",
    "# You're now specifying that responses should be in Spanish ‚Äî this gets injected into the prompt via {language}\n",
    "input_messages = [HumanMessage(query)]\n",
    "# Wrap the user's message in a HumanMessage ‚Äî required by LangChain\n",
    "\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},  #Your full custom state\n",
    "    config,  #Includes thread_id for memory persistence\n",
    ")\n",
    "#This calls your LangGraph app with:\n",
    "# - Full message history (starts with \"Hi! I'm Bob.\")\n",
    "# - The language instruction (\"Spanish\")\n",
    "# - The memory thread ID (\"abc456\")\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()\n",
    "#Prints the model's latest response ‚Äî should be in Spanish, and should remember that your name is Dan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e62767a0-12ac-43bc-a1b7-82e70b228e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Prompt Preview -----\n",
      "('messages', [SystemMessage(content='You are a helpful assistant. Answer all questions to the best of your ability in Spanish.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Hi! I'm Dan.\", additional_kwargs={}, response_metadata={}, id='47341a4b-968c-44b1-b69d-8d6fbcc8626e'), AIMessage(content='¬°Hola, Dan! ¬øC√≥mo puedo ayudarte hoy?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 34, 'total_tokens': 46, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_80cf447eee', 'id': 'chatcmpl-BN17mK1MKyZ0UPxI5chQxDxow9IVm', 'finish_reason': 'stop', 'logprobs': None}, id='run-c546ffb8-4f79-4a24-ae67-9091bb3e7af4-0', usage_metadata={'input_tokens': 34, 'output_tokens': 12, 'total_tokens': 46, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='2cb8d2d7-7a01-49c5-9868-8bcefa418a10')])\n",
      "--------------------------\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Tu nombre es Dan.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "#You‚Äôre asking the bot to recall your name ‚Äî based on prior memory\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "# Wrap your follow-up input in a HumanMessage\n",
    "\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},  #Notice: no \"language\" key this time!\n",
    "    config,  #Same thread ID, so memory and config are reused\n",
    ")\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()\n",
    "# Prints the AI's reply ‚Äî it should remember your name (e.g., Dan)\n",
    "# It should still respond in Spanish unless changed in a new input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "689ad1a6-212b-4775-b0dc-029f0a97c0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "#SystemMessage defines a system-level instruction (like setting personality or language)\n",
    "#trim_messages is a helper to manage and truncate long message histories\n",
    "\n",
    "#Create a trimmer instance with specific rules\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=150,          #Only keep messages that fit within 65 tokens total\n",
    "    strategy=\"last\",        #Keep the most recent messages (\"last N\" style)\n",
    "    token_counter=model,    #Use the model itself to estimate how many tokens each message uses\n",
    "    include_system=True,    #Always include the system message, if present\n",
    "    allow_partial=False,    #Don‚Äôt include partial (cut-off) messages\n",
    "    start_on=\"human\",       #Start trimming from the most recent human message backward\n",
    ")\n",
    "#Simulated message history\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),  # System instruction\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "#Apply the trimming logic to this full list of messages\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5f6b74f2-1eb0-46fa-b573-25c97ec74b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "#Re-initialize the LangGraph workflow using your custom state (with messages + language)\n",
    "\n",
    "def call_model(state: State):\n",
    "    #Trim the message history before sending it to the model\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    #This prevents the message list from growing too large for the model's context window\n",
    "\n",
    "    #Use the trimmed messages + current language setting to generate the prompt\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\n",
    "            \"messages\": trimmed_messages,      #Safe, shortened chat history\n",
    "            \"language\": state[\"language\"],     #Keep the preferred response language\n",
    "        }\n",
    "    )\n",
    "\n",
    "    #Send the prompt to the model and get a response\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    #Return the new AI message so it can be appended to memory\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "#Re-define the workflow structure\n",
    "workflow.add_edge(START, \"model\")           # Define flow: start ‚Üí model node\n",
    "workflow.add_node(\"model\", call_model)      # Attach the logic defined above\n",
    "\n",
    "#Set up in-memory conversation tracking\n",
    "memory = MemorySaver()\n",
    "\n",
    "#Compile the app with the updated call_model logic and memory system\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5bf46a96-e544-418c-8921-67b58db03b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "#Start a brand new memory thread ‚Äî no prior messages saved under this ID\n",
    "\n",
    "query = \"What is my name?\"\n",
    "language = \"English\"\n",
    "#Set the assistant's language to English (won't default to Spanish anymore)\n",
    "\n",
    "#Use the full list of sample messages from before (from trimmer test)\n",
    "#Add a new message at the end where you ask \"What is my name?\"\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "\n",
    "#Send trimmed chat history + language instruction into the chatbot\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},  # Full input state\n",
    "    config,                                              # Scoped to a new thread\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()\n",
    "#Show the assistant's response ‚Äî should say it doesn't know your name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6271110f-5a99-461c-b018-6462cc68d809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You asked, \"What's 2 + 2?\"\n"
     ]
    }
   ],
   "source": [
    "#Set up a new memory thread ‚Äî this keeps it separate from your earlier examples\n",
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "\n",
    "#User wants to know what math problem they asked earlier in the conversation\n",
    "query = \"What math problem did I ask?\"\n",
    "\n",
    "#Specify that the assistant should reply in English\n",
    "language = \"English\"\n",
    "\n",
    "#Combine the pre-defined list of messages (from earlier test) with the new query\n",
    "#The message history includes: \n",
    "# - \"what's 2 + 2\" ‚Üí \"4\"\n",
    "# - Followed by: \"thanks\" ‚Üí \"no problem!\"\n",
    "# - Then: \"having fun?\" ‚Üí \"yes!\"\n",
    "# - Finally: \"What math problem did I ask?\"\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "\n",
    "#Invoke the LangGraph app with your conversation and language instruction\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},  # Pass updated state (trimmed before prompt)\n",
    "    config,  # Thread ID to track memory and maintain consistency\n",
    ")\n",
    "\n",
    "#Print the AI's latest response ‚Äî should reference the math question you asked\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3f5e7e4c-251f-4b97-8a03-7b651b0753a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Hi| Dan|!| Here|‚Äôs| a| joke| for| you|:\n",
      "\n",
      "|Why| don't| scientists| trust| atoms|?\n",
      "\n",
      "|Because| they| make| up| everything|!||"
     ]
    }
   ],
   "source": [
    "#Start a new thread for streaming ‚Äî completely separate from earlier sessions\n",
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "\n",
    "#Initial user message\n",
    "query = \"Hi I'm Dan, please tell me a joke.\"\n",
    "language = \"English\"\n",
    "\n",
    "#Wrap the message in a HumanMessage object\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "#Use .stream() to start the LangGraph app, but stream tokens as they‚Äôre generated\n",
    "#Instead of waiting for the full model response, we get it **token by token**\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},  #Full input state\n",
    "    config,                                               #Threaded memory\n",
    "    stream_mode=\"messages\",                               #Stream raw message tokens\n",
    "):\n",
    "    #Filter: Only show chunks that are actual model messages (ignore intermediate states)\n",
    "    if isinstance(chunk, AIMessage):\n",
    "        #Print each token chunk as it arrives, followed by a \"|\"\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792f1e25-9f81-487d-9263-a436d8e1d77d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
